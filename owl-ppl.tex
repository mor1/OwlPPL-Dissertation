\documentclass[sigconf]{acmart}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%   June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
\acmSubmissionID{74}

\usepackage{subcaption}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.13}

\usepackage{placeins}

\usepackage{pifont}
\definecolor{green1}{rgb}{0.0, 0.5, 0.0}
\definecolor{red1}{rgb}{1.0, 0.01, 0.24}
\newcommand{\cmark}{\textcolor{green1}{\checkmark}}%
\newcommand{\xmark}{\textcolor{red1}{\ding{55}}}%

\usepackage[outputdir=out, cache=false]{minted}
\setminted{fontsize=\footnotesize}
\newmintedfile[ocamlcode]{ocaml}{frame=none,framesep=7pt}
\newmintedfile[jscode]{js}{frame=none,framesep=7pt}
\newmintedfile[clojurecode]{clj}{frame=none,framesep=7pt}
\newminted[ocamlcode-in]{ocaml}{frame=single,framesep=7pt,autogobble}

\usepackage{xpatch,letltxmacro}
\LetLtxMacro{\cminted}{\minted}
\let\endcminted\endminted
\xpretocmd{\cminted}{\RecustomVerbatimEnvironment{Verbatim}{BVerbatim}{}}{}{}

\usepackage{xspace}

\newcommand\note[2]{\color{#1}\bf #2}
\newcommand\mort[1]{{\note{red}{mort: #1}}}
% \newcommand\mortl[1]{{\color{red} mort: \begin{itemize}#1\end{itemize}}}

\newcommand{\one}{({\em i})\/}
\newcommand{\two}{({\em ii})\/}
\newcommand{\three}{({\em iii})\/}
\newcommand{\four}{({\em iv})\/}
\newcommand{\five}{({\em v})\/}

\newcommand{\sampling}{\emph{sampling}\xspace}
\newcommand{\s}[1]{(\S\ref{#1})}

\newcommand{\pupil}{Pupil\xspace}

\usepackage{float}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[
  \pupil, an Efficient Trace-Based, Type-Safe Probabilistic Programming Language
]{
  \pupil, an Efficient Trace-Based, Type-Safe \\
  Probabilistic Programming Language
}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Anik Roy}
\authornote{Work carried out for final year undergraduate project.}
\affiliation{%
  \institution{Christ's College}
  \city{Cambridge University}
  \country{UK}
}
\email{anik545@gmail.com}

\author{Richard Mortier}
\affiliation{%
  \institution{Department of Computer Science \& Technology}
  \city{Cambridge University}
  \country{UK}
}
\email{richard.mortier@cl.cam.ac.uk}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Creating statistical models and performing inference on these models is key to data science. A probabilistic programming language (PPL) is a language for creating complex models by composing simpler models and probability distributions, separating inference from model specification, allowing inference to be performed automatically~\cite{gordon2014probabilistic}. We present \emph{\pupil}, a shallow embedded PPL in the OCaml language that leverages OCaml's expressive type system and efficient native code generation. We compare \pupil's performance to that of two well-known alternative PPLs, Anglican and WebPPL, and find that \pupil outperforms WebPPL in inference speed and is commensurate with Anglican, and in both cases uses substantially less memory, making it particularly appropriate for use in edge computing applications.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
%   <ccs2012>
%   <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%   </concept>
%   <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%   </concept>
%   <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%   </concept>
%   <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%   </concept>
%   </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{datasets, neural networks, gaze detection, text tagging}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{s:introduction}

Creating statistical models and performing inference on these models is key to data science. Such modelling begins by formulating a prior belief over some parameters ($x$), the generative model ($p(x)$), and a set of conditions ($p(y|x)$) which specify the likelihood of observed data given the parameters. The goal is then to find the posterior, the (inferred) distribution over the parameters given the data we observe ($p(x|y)$), but  this is generally analytically intractable so approximate methods are used. Unfortunately,  maintaining a clear separation between the approximation (or inference) method and the model itself is difficult, which reduces the re-usability of methods and the robustness of implementations, making errors more likely and tying  performance optimisations to particular models rather than allowing them to be generalised across many models using the same inference method.

A probabilistic programming language (PPL) is a language for creating complex models by composing simpler models and probability distributions, separating inference from model specification, allowing inference to be performed automatically~\cite{gordon2014probabilistic}. This has numerous advantages: inference code is written and optimised once, independent of model; many different inference algorithms can be provided, suitable for different problems; and models can be written by domain experts, without concern for inference~\s{s:related}.

PPLs can be standalone languages or embedded into another language, giving access to the full power of the host language, making it easier to combine models. We present \emph{\pupil}, a shallow embedded PPL in the OCaml language that leverages OCaml's expressive type system and efficient native code generation. \pupil can represent a wide variety of models, not limited to finite graphical models or discrete distributions, and provides several inference procedures. Distributions are represented using a Generalised Algebraic Data Type (GADT) as a monad, allowing distributions to be combined type-safely to build models~\s{s:pupil}.

We show that \pupil's inference algorithms are correct using statistical tests on simple programs that can be solved analytically. We compare \pupil's performance to that of two well-known alternative PPLs, Anglican and WebPPL, We find that \pupil outperforms WebPPL in inference speed, and is commensurate with Anglican. In both cases, \pupil exhibits significantly lower memory usage making it particularly suitable for low-resource availability environments such as edge computing~\s{s:evaluation}.

The contributions of this paper are to present the design of a type-safe PPL embedded in the OCaml language, and to show that this PPL can match existing PPLs for speed while having significantly reduced memory footprint.

\section{Related Work}
\label{s:related}

\begin{table}
  \centering
  \begin{tabular}{ l l c c l }
    \toprule
    \textbf{PPL}
    & \textbf{Host}
    & \textbf{Universal?}
    & \textbf{Continuous?}
    & \textbf{Year} \\
    \midrule

    BUGS~\cite{gilks1994bugs}
    & --- & \xmark & \cmark & 1994 \\

    IBAL~\cite{ibal}
    & OCaml & \xmark & \xmark & 2000 \\

    JAGS~\cite{plummer2004jags}
    & --- & \xmark & \cmark & 2004 \\

    Erwig~\cite{erwig}
    & Haskell & \cmark & \cmark & 2006\\

    Church~\cite{goodman2012church}
    & LISP & \cmark & \cmark & 2008 \\

    HANSEI~\cite{kiselyov2009embedded}
    & OCaml & \xmark & \xmark & 2009 \\

    Infer.NET~\cite{wang2011using}
    & F\# & \xmark & \cmark & 2011 \\

    STAN~\cite{carpenter2017stan}
    & --- & \xmark & \cmark & 2012 \\

    Anglican~\cite{anglican-smc}
    & Clojure & \cmark & \cmark & 2014 \\

    Edward~\cite{edward}
    & --- & \xmark & \cmark & 2017\\

    WebPPL~\cite{mobus2018structure}
    & JavaScript & \cmark & \cmark & 2018 \\

    Pyro~\cite{bingham2019pyro}
    & Python & \cmark & \cmark & 2019 \\

    \pupil
    & OCaml & \cmark & \cmark & 2020 \\
    \bottomrule
  \end{tabular}
  \caption{\label{tab:ppl-summ}
    A selection of current PPLs and their significant attributes.}
\end{table}

There are many examples of PPLs, both as DSLs embedded into other languages (including OCaml) and as standalone compilers. Standalone languages have their own syntax and compiler, so can be fine tuned to the task of inference, but often lack features since they have to be built from scratch. Embedded languages can utilise facilities in their host language, such as type checking, compilers or libraries, as well as allowing programs to be integrated into existing systems easily, but they must work around the syntax and semantics of the host language. Some early PPLs, such as BUGS~\cite{gilks1994bugs} or JAGS~\cite{plummer2004jags}, limited the types of models representable in the language to finite graphical models, where the model could be expressed as a static graph of random variables and their relationships.

Many languages restrict the set of allowed models in order to use more efficient inference algorithms which can take advantage of the restricted structure of models. Universal languages can represent any model, but suffer from less predictable inference procedures since many properties of the model (such as the number of random variables) are not available at compile-time. Restricting the types of possible models can lead to efficient implementations of inference algorithms. Languages such as STAN~\cite{carpenter2017stan} or Infer.NET~\cite{wang2011using} exploit this, and forbid, e.g.,~unbounded recursion when defining models.

PPLs which can express models that have an unlimited number of random variables (and so do not compile to a static graph) are known as `universal'~\cite{borgstrom2016lambda}, and include Church~\cite{goodman2012church}, WebPPL~\cite{mobus2018structure} and Anglican~\cite{anglican-smc}. These tend to be slower at inference due to the need to support a greater range of models. Some PPLs restrict the types of distribution allowed, for example HANSEI~\cite{kiselyov2009embedded} and IBAL~\cite{ibal} only allow discrete distributions.

There are two principle approaches to the implementation of PPLs. \one~Graph-based, where a finite graph representing the variables and their relationships, over which efficient inference can take place, is generated from a program, e.g,~Infer.NET~\cite{wang2011using} or JAGS~\cite{plummer2004jags}. It has the benefit of being able to process high-dimensional data well as efficient computation graph frameworks can be leveraged, e.g.,~Edward~\cite{edward}, which uses TensorFlow~\cite{tensorflow} as a backend. However, it does restrict the types of models to those that can be represented by the underlying graph. \two~Trace-based, where execution traces corresponding to each run of a program with intermediate random variables taking a particular value, are reasoned over by inference algorithms to produce a posterior distribution~\cite{anglican-smc,mobus2018structure}. This can lead to greater expressiveness as we are not limited by the constraints of a graph, but inference is often slower as more general purpose algorithms must be used.

Prior PPLs embedded in OCaml include IBAL~\cite{ibal} and HANSEI~\cite{kiselyov2009embedded}, and \pupil takes some inspiration from these particularly in implementation of efficient inference engines. A summary of several PPLs is given in Table \ref{tab:ppl-summ}.

\section{\pupil, an OCaml PPL}
\label{s:pupil}

Shallowly embedded in the OCaml language, \pupil uses algebraic datatypes to represent probabilistic programs as trees and pattern matching to simplify interpretation and transformation of these trees, while supporting all the usual OCaml language features (e.g.,~branching, higher-order functions, let-bindings) and libraries. \pupil builds on Owl, a scientific computing library written for OCaml~\cite{owl} containing functions for working with a wide variety of probability distributions, as well as to find their probability density function (pdf) and to sample efficiently from them, the functions required to perform inference. While support for recursion includes  recursively defined models which can be non-terminating (and therefore invalid), we can write functions which are \textit{stochastic recursive}~\cite{siegmund}, i.e.,~have a probability of termination tending to 1 as the number of successive calls tends to infinity.

Two operations provide the shallow embedding: \one~\emph{sample}, for taking a sample from a distribution; and \two~\emph{condition} (often called \emph{observe} or \emph{score} in other PPLs), for conditioning on observations, defining how likely is observed data . The design challenge is to model the nondeterminism in \emph{sample} and integrate the information from \emph{condition} to guide inference. Most universal PPLs use a feature that enables exploring subcomputations -- the different execution traces whether using continuation passing style (CPS) transformations, e.g.,~WebPPL and Church~\cite{mobus2018structure,goodman2012church}, or algebraic effects, e.g.,~Pyro~\cite{bingham2019pyro}. In \pupil we model conditional distributions as monads~\cite{scibior2015practical}, and realise probabilistic programs as a GADT.

\subsection{Monads}

A design pattern commonly used in functional programming languages, \emph{monads} ``wrap'' values by defining two operations, \one~\emph{return}, taking a value and returning a monad, and \two~\emph{bind}, taking a monad and a function, and applying the function to the value wrapped inside the monad, returning the wrapped result.\footnote{Monads must also satisfy a set of laws, omitted here for space~\cite{wadler1990comprehending}.} Monads can be useful in structuring programs, allowing side effects to be captured in types.

It has been shown that probability distributions form a monad~\cite{giry1982categorical, jones1989probabilistic}, and can be used to create distributions composed from other distributions~\cite{ramsey2002stochastic}. In this case, \emph{return x} represents a distribution with only one value (\emph{x}, a Dirac distribution) while \emph{bind d f} composes distributions by taking the output of one distribution (\emph{d}) and using it in the body of the function (\emph{f}). Note that calling \emph{bind} does not produce a sample but exposes the structure to an interpreter (the inference engine) which can subsequently decide what to do.

Monads also allow use of the extended \emph{let} operators introduced in OCaml 4.08 which allow sampling from a distribution in a model to be done using the \emph{let*} operator where the variable so bound can be used as if it were a normal value. The one caveat is that the user must remember to \emph{return} at the end of the model with whatever variable(s) they want to find the posterior over. Similarly, the \emph{and*} operator can be used to enable more efficient sampling and inference when we use several independent distributions in a row by encoding that structure.

\subsection{GADTs}

Many underlying data structure can be used to represent distributions as monads. A simple example is a list of pairs representing a set of values and their corresponding probabilities, a natural way to represent discrete distributions~\cite{erwig}. However, this cannot be used to represent continuous distributions and inference is not efficient as no information from the model (e.g.,~how random variables are combined or from what distributions they came) is encoded in this representation.

\begin{figure}
  \centering
  \begin{subfigure}[t]{\columnwidth}
    \ocamlcode{code_snippets/adt_prep.ml}
    \caption{\label{lst:adt}The type of a simple language using an ADT.}
  \end{subfigure}
  \quad\\
  \begin{subfigure}[t]{\columnwidth}
    \ocamlcode{code_snippets/gadt_prep.ml}
    \caption{\label{lst:gadteg}The type of a simple language using a GADT.}
  \end{subfigure}
  \caption{\label{lst:adtgadt}Implementations of the type for a simple language using ADTs and GADTS. The GADT allows the type checker to refuse bad expressions where the ADT cannot.}
\end{figure}

The main data structure I use to represent distributions is a Generalised Algebraic Data Type (GADT). As with ADTs (sum types) they have a set of constructors, but they can have their output types annotated with different return types, making them more general than ADTs whose return types are all the same. Figure~\ref{lst:adtgadt} shows the type for  a simple language comprising floats, booleans, and pairs of floats can be represented using ADTs (Figure~\ref{lst:adt}) and GADTs (Figure~\ref{lst:gadteg}). In the former case, the type checker is unable to prevent construction of invalid values such as \texttt{Add(F 0.5, B true)} because all of \texttt{F 0.5}, \texttt{B true} and \texttt{Add(..., ...)} are typed as \texttt{'a expr}. In the latter, the type checker knows the two values in the \texttt{Add} must be \texttt{float expr} and the result is a \texttt{float expr}.

GADTs are often used to implement interpreters in functional languages, and have been used to represent probabilistic models~\cite{scibior2015practical} that can then be `interpreted' by a sampler or inference algorithm by traversing the model, ignoring conditionals to enable forward sampling from the prior. Inference is provided for by transforming conditional distributions to remove conditional statements allowing sampling to be performed as normal, or by generating an empirical distribution that can be sampled from similarly.

\begin{figure}
  \small
  \ocamlcode{code_snippets/gadt.ml}
  \caption{\label{lst:gadt}Representing a probabilistic model using a GADT}
\end{figure}

Figure~\ref{lst:gadt} shows the GADT variants and the monad functions that construct them: \emph{Return} represents a distribution with one value; \emph{Bind} contains a distribution and a function representing the application of the function to the output from that distribution (also bound to \emph{let*}); \emph{Independent} represents models with independent sub-parts, such as when drawing samples from many independently distributed variables, and can be used to parallelise models (also bound to \emph{and*}); \emph{Primitive} represents distributions of a type for which can find the exact pdf/cdf, while the general \emph{dist} type can only be sampled; and \emph{Conditional} assigns scores (likelihoods) to execution traces via a function that takes an element produced by a model and returns a score for the corresponding trace. An important feature of this type is that it is polymorphic: distributions can be defined over any type, including arbitary ADTs or even other distributions.

\subsection{Representing Distributions}
\pupil uses three different data structures to represent the different types of distribution: \one~\emph{input distributions}, primitive distributions used to build models, \two\emph{output distributions}, empirical distributions built from a set of samples from a posterior, and \three~\emph{general probabilistic models}, composed primitives conditioned on data.

\paragraph{Input Distributions}
Complex models are built by composing distributions about which we have extra information such as exact equations and the ability to sample directly. Such primitive distributions will have operations such as \emph{sample}, \emph{pdf}, \emph{cdf} defined, giving the standard properties of distributions, used to perform inference. \pupil also allows users to define their own primitive distributions by simply providing the necessary set of operations.

\paragraph{Output Distributions}
Bayesian inference produces probability distributions over the variables of interest, ideally the exact posterior distribution -- but approximate inference only allows us to create functions to sample from this posterior. For discrete distributions, we use a \emph{Core.Map}\footnote{\url{https://ocaml.janestreet.com/ocaml-core/latest/doc/base/Base/Map/index.html}}, with the values that the distribution can take and the number of samples of each value as the keys and values of the map. For continuous distributions we use a dynamically resizing array so that adding each sample is $O(1)$ amortised, with statistics calculated using Owl's functions that operate on arrays.

\paragraph{General Probabilistic Models}
By including the \emph{Conditional} variant, the above GADT can be used to describe general models by incorporating observed data into our models and performing inference. This variant represents observations by assigning scores to traces using a function that takes an element and returns a score representing how likely the current trace is given the value passed to the function. Different inference algorithms will use this information to produce a distribution over all possible traces, and different forms of conditioning are provided: \one~\emph{hard}, which scores the model 1 or 0 if an observation is true or false,  representing how likely it is for the current trace to occur and allowing certain variables or outcomes in a model to be constrained; \two~\emph{soft}, using the distribution's pdf to determine the likelihood of the observation in the model; and \three~a \emph{constant}, used in branching statements, where a score is assigned typically based on some deterministic) condition.

PPLs allow us to create these models as programs in code. Generative models are built by taking samples from probability distributions, so PPLs need some way of modelling this non-determinism. Being able to condition the values of variables on data is the other key part of PPLs, since we are interested in the posterior, which is conditional on the data. Without conditioning, we can run a program forwards, which generates samples from the model we write (the prior). By taking into account conditioning, we can infer the distribution of the input parameters based on the data we observe and sample from this distribution.

We can also take into account the conditionals, and produce weighted samples, with the weight being the score assigned by each conditional branch, accumulated by multiplying all the scores. This gives us a set of values with corresponding weights which represent how likely those values are. An important property of these weights is that they are not normalised, so we cannot use them to find the posterior directly.

\begin{figure}
  \begin{subfigure}[t]{\linewidth}
    \ocamlcode{code_snippets/linreg.ml}
    \caption{\label{lst:linreg:pupil}\pupil}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{\linewidth}
    \jscode{code_snippets/webppl/linreg.js}
    \caption{\label{lst:linreg:webppl}WebPPL}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{\linewidth}
    \clojurecode{code_snippets/anglican/linreg.clj}
    \caption{\label{lst:linreg:anglican}Anglican}
  \end{subfigure}
  \caption{\label{lst:linreg}Comparing linear regression implementations.}
\end{figure}
\begin{figure*}
  \centering
  \begin{subfigure}[t]{\textwidth}
    \centering
    \input{tikz/times_plot.tex}
    \caption{\label{fig:time-perf}
      Inference time.}
  \end{subfigure}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \input{tikz/mems_plot.tex}
    \caption{\label{fig:mem-perf}
      Memory usage.}
  \end{subfigure}
  \caption{\pupil performance on different models, taking 10,000 samples from the posterior and averaging over 20 runs. Results shown for Metropolis-Hastings (\emph{mh}), Bootstrap Particle Filter (\emph{smc}), and Rejection Sampling (\emph{rej}). Error bars show the 95\% confidence interval. }
\end{figure*}


\subsection{Inference}
Inference can be thought of as a program transformation~\cite{scibior2015practical, Zinkov2016ComposingIA}, corresponding in \pupil to a function of type \emph{'a dist -> 'a dist}, which allows for the composition of inference algorithms. \pupil implements a range of inference algorithms of which we describe those used in the evaluation~\s{s:evaluation}; others include Likelihood Weighting (Importance Sampling), Particle Cascade SMC, and Particle-Independent Metropolis-Hastings (PMCMC).

\paragraph{Exact Inference}
The simplest but usually computationally intractable (due to the normalising constant) method for calculating the posterior by calculating Bayes formula exactly is \emph{exact inference}. For discrete posterior distributions it can be thought of as calculating the probability of every possible value of the variable of interest, naive and very inefficient. As it essentially considers every possible execution trace, we do not exploit structure such as overlapping traces. It can be made slightly more efficient by using algorithms such as belief propagation~\cite{belief-prop}, but they still only work on models made up from discrete distributions. Exact inference of this kind only works on models that can be represented as finite networks, and for Bayesian networks is in fact NP-hard~\cite{cooper1990computational}, so most methods focus on approximate inference.

\paragraph{Rejection Sampling}
As exact inference is too difficult in practice, \emph{Monte Carlo} methods~\cite{monte-carlo} are often used, relying on repeated sampling to infer distribution properties. One such is \emph{Rejection Sampling} which simply takes samples from a `proposal' distribution that {\bf can} be sampled from, accepting or rejecting them based on the pdf of the proposal distribution. It can be shown that samples taken using this method converge to the required distribution~\cite{flury1990acceptance} but it is naive, running an entire trace even if the first condition dropped the score below the threshold. \pupil slightly optimises by short-circuiting this, rejecting as soon as the trace goes below the threshold. Nonetheless, if conditions make most execution traces very unlikely, it will take a very large number of samples to have enough (or any) accepted samples.

\paragraph{Metropolis-Hastings}
Monte Carlo Markov Chains (MCMC) methods involve constructing a Markov chain with a stationary distribution equal to the posterior distribution. A Markov chain is a statistical model consisting of a sequence of events, where the probability of any event depends only on the previous event. The stationary distribution is the distribution over successive states to which the chain converges (if it converges). \emph{Metropolis Hastings} is an MCMC algorithm used to find a Markov chain with the stationary distribution equal to the target distribution, here the posterior. \pupil implements the Independent Metropolis Hastings (IMH) algorithm.

\paragraph{Sequential Monte Carlo}
Large numbers of weighted samples are used to represent a posterior distribution in \emph{Sequential Monte Carlo} (SMC) methods; the samples are sometimes termed \emph{particles} and SMC methods known as \emph{Particle Filters.} A particle is a value paired with a non-normalised weight representing the likelihood of that value in the distribution. These particles are updated when data is observed and re-sampled from in order to converge the set of particles to the posterior. The simplest SMC algorithms are \emph{Particle Filters}~\cite{particlefilter}, which simply resample particles on encountering new data, sequentially updating the weights of the particles as we observe condition statements (i.e.~data) based on how likely this data is deemed to be.

The GADT is traversed top down, with particles being initialised at a `leaf' - primitives or returns. From this root, bind functions apply functions to the particles, and conditional statements updates the weights and resamples. The \emph{resample} function takes a set of particles and takes samples from this set with replacement - this is the `bootstrap' resampling method. The output distribution is conditioned by the total weight of all particles. Increasing the number of particles finds a more accurate distribution with a finer resolution, but also increases the amount of time and memory required.

\section{Evaluation}
\label{s:evaluation}

Testing inherently statistical code is more complex than deterministic code. One could fix a random seed, ensuring the same sequence of results are produced but this is overly strict: we desire only that samples fit a distribution not that the same sequence is produced. A better approach is to perform hypothesis testing using, e.g.,~Kolmogorov-Smirnov~\cite{massey1951kolmogorov}, to ensure the correct distributions are produced but such tests are correctly expected to fail with a given rate making them unsuitable for unit testing. Using $\chi^2$ and Kolmogorov-Smirnov (K-S) tests of similarity between distributions by comparing the empirical distribution of 10,000 samples from an approximation to an exact distribution ($\alpha = 0.05$) we find that all test statistics exceed 0.05 so we do not reject the null hypothesis, i.e.,~we consider the distributions are not significantly different.

We use four example problems in our evaluation.

% \begin{figure}
%   \centering
%   \includegraphics[height=1.5in]{figs/sprinkler-network.png}
%   \caption{\label{fig:sprinkler-network}Sprinkler model as a network.}
% \end{figure}
\paragraph{Sprinkler}
The sprinkler model is an example of exact inference on a discrete model represented by a Bayesian network, % (Figure~\ref{fig:sprinkler-network})
a commonly used simple example in Bayesian inference. We model the probability of rain given that the grass is wet, itself dependent on whether or not it is raining or the sprinkler is on, and the probability of rain and the sprinkler being on depend on whether it is cloudy.

\paragraph{Biased Coin}
An example of exact inference on a continuous model involves a coin being flipped $n$ times that lands on heads $x$ times, where we seek the distribution over the weight of the coin (i.e., how likely it is to land on heads again). The likelihood model is a binomial, $X \sim \text{Binom}(n,\theta)$, and the prior is uninformative, $\Theta \sim \text{Uniform}(0,1)$.

\paragraph{Hidden Markov Models}
Hidden Markov Models (HMMs) are more complex, having a sequence of hidden states that emit observed states. There are two distributions involved, whether discrete or continuous: \one~the \emph{transition distribution} defining how likely the next state is given the current state, and \two~the \emph{emission distribution} over the observed states given the hidden state. Our model uses discrete distributions, with hidden and observed states \emph{True} and \emph{False}, starting state \emph{True}, and transition matrix
$T = \big(\begin{smallmatrix}
  0.7 & 0.3 \\
  0.3 & 0.7
\end{smallmatrix}\big)$
and emission matrix,
$O = \big(\begin{smallmatrix}
  0.9 & 0.1 \\
  0.1 & 0.9
\end{smallmatrix}\big)$.

\paragraph{Linear Regression}
Finally, this example uses multiple 2-D data points to infer a continuous (straight line) distribution$ y=\beta_1 x_1 + \beta_0 + \epsilon$ where  $\beta_i$ are the terms to be determined, and $\epsilon$ represents random error. A frequentist approach minimises the sum of least squares between the known values and the predicted outputs to find a single best set of values for the parameters. In Bayesian linear regression, we also use prior distributions to augment the data:   $y \sim N(\beta_0 + \beta_1 x_1 + \epsilon, 1.)$ as the likelihood model and $\beta_0 \sim N(0,1)$ and $\beta_1 \sim N(0,1)$ the priors over the slope and $y$-intercept.

As an example, Figure~\ref{lst:linreg} compares implementation of the linear regression model in the three languages used for evaluation: \pupil, Anglican (embedded in Clojure) and WebPPL (embedded in JavaScript).

All tests are carried out on a single core of an Intel\textsuperscript{(R)} Core\textsuperscript{(TM)} i5-7200U CPU @ 2.50GHz. We compare the performance of \pupil against Anglican and WebPPL, well-known universal PPLs embedded in different host languages, for those inference algorithms for which there are comparable implementations. Figures~\ref{fig:time-perf} and~\ref{fig:mem-perf} compares \pupil against these languages for a range of models and inference procedures. We consider both running time and peak memory usage. We see that \pupil performs consistently better that WebPPL in both memory and time, perhaps because it uses NodeJS incurring interpretation overheads. It also exhibits less variance, except when using rejection sampling for the Coin model. \pupil slightly outperforms Anglican on the two continuous models except when using rejection sampling (the current implementation is quite naive) but is generally slower than Anglican for the discrete models. However, \pupil outperforms both Anglican and WebPPL in terms of peak memory usage. Although all three languages are garbage collected, OCaml generates native binaries rather than relying on a virtual machine, and so does not incur the overheads introduced by the JVM (used by Clojure, Anglican's implementation language) and the NodeJS runtimes.

\begin{figure}
  \centering
  \input{tikz/linreg_by_datasize.tex}
  \caption{\label{fig:time-datasize}
    Time taken for inference as a function of input data length as the mean of 10 runs each taking 1,000 samples from the posterior, shaded areas are the 95\% confidence interval ($\pm 2\sigma$)}
\end{figure}

It is also important to consider running time of inference as more data is used, as  models conditioned on more data should  give more accurate results. We evaluated  the linear regression model, increasing the length of the array used as input, and see that running time increases linearly with the size of data. Figure~\ref{fig:time-datasize} shows all inference functions running in time linear to the size of the input, albeit with substantial variation in constant factors, e.g.,~Sequential Monte Carlo has a much steeper gradient than Metropolis-Hastings.

\begin{figure*}
  \centering
  \input{tikz/kl_plot.tex}
  \caption{\label{fig:kl}
    Plot of KL-divergence with increasing number of samples for different models and inference procedures, averaged over 20 runs. We use log-log axes  due to large  variation between  the curves, which do map to straight lines on a linear axes.}
\end{figure*}

Figure~\ref{fig:kl} shows the trade-off between speed and accuracy: the particle filter is slower than rejection sampling here but produces more accurate posterior distributions. For each inference procedure, we can see that the KL-divergence for each model generally decreases as we take more samples. Rejection sampling is consistently the worst performing inference procedure, with particle based methods such as the particle filter or importance sampling generating more accurate distributions. These plots have all been smoothed by taking a moving average in order to reduce the impact of noise.

\section{Conclusion}
\label{s:conclusion}

We have presented the design and evaluation of \pupil, a universal probabilistic programming language shallowly embedded in OCaml. It can represent a wide variety of models, including infinite models with unbounded recursion, while supporting all the standard OCaml language features such as pattern matching or higher order functions. It allows models to be combined in complex ways, and existing OCaml libraries can be used within those models. Its performance is competitive with other universal PPLs, particularly the memory usage which is significantly lower than WebPPL and Anglican -- which may make \pupil particularly appropriate for edge computing environments. \pupil is publicly available,\footnote{\url{http://github.com/anik545/OwlPPL.git}} and we welcome improvements and further benchmarks -- we are particularly interested in starting to use the recently released PPL Bench~\cite{pplbench}.

% \begin{acks}
% \end{acks}

{
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{owl-ppl.bib}
}

\end{document}
